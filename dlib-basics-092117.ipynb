{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from imutils import face_utils\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import imutils\n",
    "import dlib\n",
    "import cv2\n",
    "import os, subprocess, csv, glob\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installs**\n",
    "\n",
    "Several installs are required to use the packages in this notebook. The cv2 package can be installed most easily through conda install: \"conda install -c menpo cv3.\" This is somewhat confusingly called \"cv3\", apparently to indicate its compatibility with python 3.x, even though it is still imported as cv2.\n",
    "\n",
    "The dlib install instructions can be found at http://www.pyimagesearch.com/2017/03/27/how-to-install-dlib/ . Imutils is easier to install with pip; simply do \"pip install imutils\".\n",
    "\n",
    "These installs would simply be included in the notebook, but doing the dlib installs actually takes quite a while (5-10 minutes) and a fairly large amount of disk space (~1GB)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Required files**\n",
    "\n",
    "Beyond the installs, dlib needs a face training data set in order for the landmark predictor to be instantiated successfully. The 68-point frontal face dataset that is standard for use of dlib's landmark predictor can be found at https://github.com/davisking/dlib-models. (This comes as a .bz2 archive and needs to be uncompressed before use.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example usage of dlib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# instantiate face detector and landmark predictor (TODO outside of fcn)\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two functions are heavily inspired by: http://codegists.com/snippet/python/facial_landmark_generatorpy_habanoz_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect_landmarks(my_ndarray):\n",
    "    \"\"\"\n",
    "    Input: an ndarray frame output from cv2.VideoCapture object.\n",
    "    Output: a (68,2) ndarray containing X,Y coordinates for the 68 face points dlib detects.\n",
    "    \"\"\"\n",
    "\n",
    "    # read in image TODO change to something more general like the commented-out line\n",
    "    gray = cv2.cvtColor(my_ndarray, cv2.COLOR_BGR2GRAY)\n",
    "    # gray = np.asarray(cv2image, dtype=np.uint8)\n",
    "    \n",
    "    # TODO cheekpad obliteration happens here if remove_cheekpad=True\n",
    "    \n",
    "    # run face detector to get bounding rectangle\n",
    "    # TODO pass detector object into function\n"
    "    rect = detector(gray, 1)[0]\n",
    "    \n",
    "    # run landmark prediction on portion of image in face rectangle; output\n",
    "    # TODO pass predictor object into function\n"
    "    shape = predictor(gray, rect)\n",
    "    shape_np = face_utils.shape_to_np(shape)\n",
    "    \n",
    "    return shape_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_landmarks(cv2_video_capture, shape, aperture_xy = False):\n",
    "    \"\"\"\n",
    "    Inputs: an ndarray frame output from a cv2.VideoCapture object, and a (68,2) ndarray of x,y coords that dlib detects.\n",
    "    Outputs: an image with lines drawn over the detected landmarks; useful for testing and visualization.\n",
    "    aperture_xy: if True, also draw (next to face) numerical values for x and y diameters of lip aperture.\n",
    "    \"\"\"\n",
    "\n",
    "    out_image = cv2_video_capture.copy()\n",
    "\n",
    "    for i,name in enumerate(face_utils.FACIAL_LANDMARKS_IDXS.keys()):\n",
    "        if name == \"mouth\":\n",
    "            continue\n",
    "        j,k = face_utils.FACIAL_LANDMARKS_IDXS[name]\n",
    "        pts = np.array(shape[j:k], dtype=np.uint32)\n",
    "        for idx,pt in enumerate(pts):\n",
    "            pt1 = pt\n",
    "            try:\n",
    "                pt2 = pts[idx+1]\n",
    "            except IndexError:\n",
    "                if name == \"left_eye\" or name == \"right_eye\":\n",
    "                    pt2 = pts[0]\n",
    "                else:\n",
    "                    continue\n",
    "            cv2.line(out_image, tuple(pt1), tuple(pt2), (255,255,255))\n",
    "    \n",
    "    # drawing the mouth with some more precision\n",
    "    # draw most of the outer perimeter of lips\n",
    "    jm,km = face_utils.FACIAL_LANDMARKS_IDXS['mouth']\n",
    "    for idx in range(jm,jm+11): \n",
    "        pt1 = shape[idx]\n",
    "        pt2 = shape[idx+1]\n",
    "        cv2.line(out_image, tuple(pt1), tuple(pt2), (255,255,255))\n",
    "    \n",
    "    # draw the last segment for the outer perimiter of lips\n",
    "    cv2.line(out_image, tuple(shape[48]), tuple(shape[59]), (255,255,255))\n",
    "    \n",
    "    # draw the inner aperture of the lips\n",
    "    for idx in range(jm+12,km):\n",
    "        pt1 = shape[idx]\n",
    "        try:\n",
    "            pt2 = shape[idx+1]\n",
    "        except IndexError:\n",
    "            pt2 = shape[jm+12]\n",
    "        cv2.line(out_image, tuple(pt1), tuple(pt2), (255,255,255))\n",
    "        \n",
    "    # add text indicating measured lip aperture in px\n",
    "    if aperture_xy:\n",
    "        x,y = get_lip_aperture(shape)\n",
    "        add_string = \"x={}, y={}\".format(round(x,1),round(y,1))\n",
    "        loc = tuple(np.subtract(shape[4], (200,0)))\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(out_image, add_string, loc, font, 0.8, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return out_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining functions are not currently implemented but suggest the outlines of future processing workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_video_frame(video, time):\n",
    "    # return a single desired frame corresponding to a given timepoint, with no annotation.\n",
    "    # would require the entire video to be streamed in. TODO make this faster?\n",
    "    # can then run draw_landmarks or get_lip_aperture on the frame.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lip_aperture(shape):\n",
    "    \"\"\"\n",
    "    Inputs: the typical 68,2 ndarray shape object output by detect_landmarks.\n",
    "    Outputs: a 2-tuple of horizontal and vertical diameters of the lip aperture,\n",
    "     treating the horizontal line like the major axis of an ellipse,\n",
    "     and the vertical line like the minor axis.\n",
    "    \"\"\"\n",
    "    horizontal_axis = np.linalg.norm(shape[60] - shape[64])\n",
    "    vertical_axis = np.linalg.norm(shape[62] - shape[66])\n",
    "    \n",
    "    return horizontal_axis,vertical_axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together: this will plot a single frame in-line (if the source file exists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO verify that the videoCapture object exists; cell below deletes all .bmp files\n",
    "testpath = 'img_0132.bmp'\n",
    "movie = cv2.VideoCapture(testpath)\n",
    "ret,test = movie.read()\n",
    "print(type(test))\n",
    "shape = detect_landmarks(test)\n",
    "out_image = draw_landmarks(test, shape)\n",
    "\n",
    "cv2.imwrite('testtest.bmp', out_image)\n",
    "plt.imshow(cv2.cvtColor(out_image, cv2.COLOR_BGR2RGB))\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell clips a short stretch of video from the longer videos typical of ultrasound acquisitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp_movie = '12-video/SN12_AA032001.MXF'\n",
    "exp_snippet = '12-video/SN12_AA032001_short.MXF'\n",
    "fname = os.path.split(exp_movie)[1]\n",
    "basename = os.path.splitext(fname)[0]\n",
    "\n",
    "# get a snippet of the MXF\n",
    "snippet_args = ['ffmpeg', '-ss', '00:00:30', \n",
    "                '-i', exp_movie, \n",
    "                \"-t\", \"00:00:08\", \n",
    "                \"-vcodec\", \"copy\", \n",
    "                \"-acodec\", \"copy\", \n",
    "                exp_snippet]\n",
    "subprocess.check_call(snippet_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this cell takes a short clip and creates (and saves) a GIF with the facial landmarks drawn over the subject's face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_movie = 'vowels/POO1.MXF'\n",
    "fname = os.path.split(exp_movie)[1]\n",
    "basename = os.path.splitext(fname)[0]\n",
    "\n",
    "subprocess.check_call(['ffmpeg','-loglevel','8','-i',exp_snippet,'-vf','fps=20','img_%05d_f.bmp'])\n",
    "movie = cv2.VideoCapture('img_%05d_f.bmp')\n",
    "\n",
    "frame_num = 1\n",
    "\n",
    "while(movie.isOpened()):\n",
    "    ret, frame = movie.read()\n",
    "    if (ret==False):   # the file is finished\n",
    "        break\n",
    "\n",
    "    # detect face region and draw landmarks on the image.\n",
    "    shape = detect_landmarks(frame)\n",
    "    out_image = draw_landmarks(frame,shape)\n",
    "    \n",
    "    # write the image to a .bmp file, with zero-padding to ensure the frames are input \n",
    "    cv2.imwrite('{0:05d}g.bmp'.format(frame_num), out_image)\n",
    "    frame_num += 1\n",
    "        \n",
    "# cleanup\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# make a gif (a bit slowed down by default) from the output bmps\n",
    "subprocess.check_call(['convert', '*g.bmp', basename+'.gif'])\n",
    "\n",
    "# remove the input bmps\n",
    "for bmp in glob.glob(\"*g.bmp\"):\n",
    "    os.remove(bmp)\n",
    "for bmp in glob.glob(\"*f.bmp\"):\n",
    "    os.remove(bmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
