{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from imgphon import helmet \n",
    "from imgphon import imgphon as iph\n",
    "import os, subprocess, glob, re\n",
    "import numpy as np\n",
    "import audiolabel\n",
    "import cv2\n",
    "import dlib\n",
    "from imutils.face_utils import FaceAligner\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [x*2 for x in plt.rcParams['figure.figsize']] # double plot window size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tilt normalization on lips directly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tSNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "n_tsne_comp = 2\n",
    "X_embedded = TSNE(n_components=n_tsne_comp).fit_transform(analysis)\n",
    "\n",
    "X_df = pd.DataFrame(X_embedded)\n",
    "X_df.columns = [\"D{}\".format(d+1) for d in range(n_tsne_comp)]\n",
    "X_df = X_df.assign(phone=pd.Series(md['uniquePhone']).values)\n",
    "df = df.assign(phase=pd.Series(md['phase']).values)\n",
    "X_df.head(5)\n",
    "sns.lmplot(x=\"D1\", y=\"D2\", data=X_df, \n",
    "             hue=\"phone\", fit_reg=False, size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct manipulations of video files (legacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract and display a single landmark-annotated video frame\n",
    "video = 'SN12_AA032001.MXF'\n",
    "time = '00:00:10.900'\n",
    "\n",
    "frame = get_video_frame(video,time)\n",
    "shape = detect_landmarks(frame)\n",
    "color_corrected = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(draw_landmarks(color_corrected,shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract a frame corresponding to the desired timepoint of an audio file (here, from a TextGrid) \n",
    "# and extracts a polygon of the lip shape\n",
    "babb = 'vowels/P1.MXF'\n",
    "babb_tg = os.path.splitext(babb)[0] + \".TextGrid\"\n",
    "pm = audiolabel.LabelManager(from_file=babb_tg, from_type=\"praat\")\n",
    "v = pm.tier('phone').search(vre)[0] # should only be one match per file; TODO check while running\n",
    "midpoint= v.center\n",
    "ffmpeg_time = str(round(midpoint,3))\n",
    "frame = get_video_frame(babb,ffmpeg_time)\n",
    "shape = detect_landmarks(frame)\n",
    "color_corrected = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(draw_landmarks(color_corrected,shape))\n",
    "\n",
    "image = lip_mask(frame,shape)\n",
    "plt.imshow(crop_center(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clip a short stretch of video from the longer videos typical of ultrasound acquisitions\n",
    "exp_movie = '12-video/SN12_AA032001.MXF'\n",
    "exp_snippet = '12-video/SN12_AA032001_short.MXF'\n",
    "fname = os.path.split(exp_movie)[1]\n",
    "basename = os.path.splitext(fname)[0]\n",
    "\n",
    "# get a snippet of the MXF\n",
    "snippet_args = ['ffmpeg', '-ss', '00:00:30', \n",
    "                '-i', exp_movie, \n",
    "                \"-t\", \"00:00:08\", \n",
    "                \"-vcodec\", \"copy\", \n",
    "                \"-acodec\", \"copy\", \n",
    "                exp_snippet]\n",
    "subprocess.check_call(snippet_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp_movie = 'vowels/POO1.MXF'\n",
    "fname = os.path.split(exp_movie)[1]\n",
    "basename = os.path.splitext(fname)[0]\n",
    "\n",
    "subprocess.check_call(['ffmpeg','-loglevel','8','-i',exp_snippet,'-vf','fps=20','img_%05d_f.bmp'])\n",
    "movie = cv2.VideoCapture('img_%05d_f.bmp')\n",
    "\n",
    "frame_num = 1\n",
    "\n",
    "while(movie.isOpened()):\n",
    "    ret, frame = movie.read()\n",
    "    if (ret==False):   # the file is finished\n",
    "        break\n",
    "\n",
    "    # detect face region and draw landmarks on the image.\n",
    "    shape = detect_landmarks(frame)\n",
    "    out_image = draw_landmarks(frame,shape)\n",
    "    \n",
    "    # write the image to a .bmp file, with zero-padding to ensure the frames are input \n",
    "    cv2.imwrite('{0:05d}g.bmp'.format(frame_num), out_image)\n",
    "    frame_num += 1\n",
    "        \n",
    "# cleanup\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# make a gif (a bit slowed down by default) from the output bmps\n",
    "subprocess.check_call(['convert', '*g.bmp', basename+'.gif'])\n",
    "\n",
    "# remove the input bmps\n",
    "for bmp in glob.glob(\"*g.bmp\"):\n",
    "    os.remove(bmp)\n",
    "for bmp in glob.glob(\"*f.bmp\"):\n",
    "    os.remove(bmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working up an anonymizing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "det = dlib.get_frontal_face_detector()\n",
    "pred = dlib.shape_predictor('../shape_predictor_68_face_landmarks.dat')\n",
    "algn = FaceAligner(pred, desiredFaceWidth = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def landmark_plot_prep(target_dir, frame_idx):\n",
    "    bmp = os.listdir(target_dir)[frame_idx] # get -70th file in dir and ...\n",
    "    frame = cv2.imread(os.path.join(target_dir, bmp)) # read in as ndarray\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # change color map to RGB for display\n",
    "    marks = iph.detect_landmarks(frame, detector=det, predictor=pred) # detect landmarks using imgphon\n",
    "    return rgb, marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import importlib\n",
    "#importlib.reload(imgphon)\n",
    "\n",
    "rgb,marks = landmark_plot_prep('../40-lip-frames',-50) # get -50th frame for Subject 40 and...\n",
    "\n",
    "# plot raw data on left; data annotated with detected facial landmarks on right\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(rgb)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(iph.draw_landmarks(rgb,marks,anonymize=True,line_width=3))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get bottom of box using nose\n",
    "nose_start,nose_end = face_utils.FACIAL_LANDMARKS_IDXS['nose']\n",
    "max_y = max([marks[i][1] for i in range(nose_start, nose_end)]) # get lowest point in nose\n",
    "\n",
    "# get top of box using eyebrows\n",
    "eyebrows_start = face_utils.FACIAL_LANDMARKS_IDXS['right_eyebrow'][0]\n",
    "eyebrows_end = face_utils.FACIAL_LANDMARKS_IDXS['left_eyebrow'][1]\n",
    "min_y = min([marks[i][1] for i in range(eyebrows_start, eyebrows_end)])\n",
    "\n",
    "# get sides of box using jaw\n",
    "jaw_start,jaw_end = face_utils.FACIAL_LANDMARKS_IDXS['jaw']\n",
    "max_x = max([marks[i][0] for i in range(jaw_start, jaw_end)])\n",
    "min_x = min([marks[i][0] for i in range(jaw_start, jaw_end)])\n",
    "\n",
    "# pad out the blurred area\n",
    "# TODO change to integer pixel values (will throw error in notebooks)\n",
    "width = max_x - min_x\n",
    "height = max_y - min_y\n",
    "min_y -= 0.4*height\n",
    "min_x -= 0.1*width\n",
    "max_y += 0.1*height\n",
    "max_x += 0.1*width\n",
    "\n",
    "# replace the area around the selected landmarks with a blurred version of the area\n",
    "upper_face = rgb[min_y:max_y, min_x:max_x]\n",
    "blur = cv2.GaussianBlur(sub_face,(101, 101), 30)\n",
    "anon_image = rgb.copy()\n",
    "anon_image[min_y:min_y+upper_face.shape[0], min_x:min_x+upper_face.shape[1]] = blur\n",
    "plt.imshow(iph.draw_landmarks(anon_image,marks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# old get_frame function, if ever needed\n",
    "def get_frame(filename, frame_idx, frame_dim_1, frame_dim_2):\n",
    "    \"\"\"\n",
    "      Inputs: filename, index of frame, frame width and height\n",
    "      Outputs: ndarray containing frame\n",
    "    \"\"\"\n",
    "    data_fmt = 'I' * np.int(frame_dim_1 * frame_dim_2) # unsigned int (I) times dimensions of output image\n",
    "    framesize = frame_dim_1 * frame_dim_2 * 4 # multiply by 4 to get number of bytes (4 bytes/pixel)\n",
    "    frame_start = frame_idx * framesize\n",
    "    with open(filename, 'rb') as fh:\n",
    "        fh.seek(frame_start) # move pointer to start of frame\n",
    "        packed_data = fh.read(framesize) # take a frame-sized chunk at this coord\n",
    "        try:\n",
    "            unpacked_data = struct.unpack(data_fmt, packed_data) # conversion to array\n",
    "        except:\n",
    "            print(\"No data at position {}\".format(frame_idx)) # catch out-of-range indices\n",
    "            rdata = None\n",
    "        else:\n",
    "            data = np.array(unpacked_data)\n",
    "            rdata = data[np.arange(frame_dim_1 * frame_dim_2)].reshape(frame_dim_1, frame_dim_2).T\n",
    "            rdata = rdata.astype(np.uint8) # for plotting\n",
    "            \n",
    "        return(rdata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
