{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from imutils import face_utils\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import imutils\n",
    "from imutils.face_utils import FaceAligner\n",
    "from imutils.face_utils import rect_to_bb\n",
    "import dlib\n",
    "import cv2\n",
    "import os, subprocess, csv, glob\n",
    "import matplotlib.pyplot as plt\n",
    "import audiolabel\n",
    "import re\n",
    "from skimage.draw import polygon\n",
    "from scipy.ndimage.measurements import center_of_mass\n",
    "from scipy.ndimage import zoom\n",
    "% matplotlib inline\n",
    "\n",
    "vre = re.compile(\n",
    "         \"^(AA|AE|AH|AO|EH|ER|EY|IH|IY|OW|UH|UW|R|L)$\"\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installs**\n",
    "\n",
    "Several installs are required to use the packages in this notebook. The cv2 package can be installed most easily through conda install: \"conda install -c menpo cv3.\" This is somewhat confusingly called \"cv3\", apparently to indicate its compatibility with python 3.x, even though it is still imported as cv2.\n",
    "\n",
    "The dlib install instructions can be found at http://www.pyimagesearch.com/2017/03/27/how-to-install-dlib/ . Imutils is easier to install with pip; simply do \"pip install imutils\".\n",
    "\n",
    "These installs would simply be included in the notebook, but doing the dlib installs actually takes quite a while (5-10 minutes) and a fairly large amount of disk space (~1GB)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Required files**\n",
    "\n",
    "Beyond the installs, dlib needs a face training data set in order for the landmark predictor to be instantiated successfully. The 68-point frontal face dataset that is standard for use of dlib's landmark predictor can be found at https://github.com/davisking/dlib-models. (This comes as a .bz2 archive and needs to be uncompressed before use.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# instantiate face detector and landmark predictor (TODO outside of fcn)\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These first two functions are critical for any further processing. get_video_frame grabs a raw video frame from a desired time, while get_norm_video_frame applies a resizing and warping algorithm to the raw frame's face bounding box and returns only that portion. Note that this involves detecting the eye landmarks and using them to determine the angle that the face is off of the horizontal, as well as the relative size of the face or closeness to the camera. The second function is heavily inspired by the demo script at https://www.pyimagesearch.com/2017/05/22/face-alignment-with-opencv-and-python/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_video_frame(video, time):\n",
    "    \"\"\"\n",
    "    Return the single frame closest to the given timepoint. Can then run detect_landmarks on the frame, ideally\n",
    "      as an input to get_norm_face.\n",
    "    Inputs: video - an MXF file; time in seconds - format d.ddd (sec.msec), rounded to three decimal places.\n",
    "    Outputs: an ndarray image of the desired frame.\n",
    "    \"\"\"\n",
    "    output_bmp = 'temp.bmp'\n",
    "    try:\n",
    "        os.remove(output_bmp)\n",
    "    except OSError:\n",
    "        pass\n",
    "    frame_get_args = ['ffmpeg', '-i', video, \n",
    "                      '-vcodec', 'bmp', \n",
    "                      '-ss', time,\n",
    "                      '-vframes', '1', \n",
    "                      '-an', '-f', 'rawvideo',\n",
    "                       output_bmp]\n",
    "    subprocess.check_call(frame_get_args)\n",
    "    frame = cv2.imread(output_bmp)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect_landmarks(my_ndarray, detector=detector, predictor=predictor):\n",
    "    \"\"\"\n",
    "    Inputs: an ndarray frame output from cv2.VideoCapture object, \n",
    "            a detector of choice from dlib,\n",
    "            and a dlib face landmark predictor trained on data of choice.\n",
    "    Output: a (68,2) ndarray containing X,Y coordinates for the 68 face points dlib detects.\n",
    "    \"\"\"\n",
    "\n",
    "    # read in image TODO change to something more general like the commented-out line\n",
    "    gray = cv2.cvtColor(my_ndarray, cv2.COLOR_BGR2GRAY)\n",
    "    # gray = np.asarray(cv2image, dtype=np.uint8)\n",
    "    \n",
    "    # TODO cheekpad obliteration happens here if remove_cheekpad=True\n",
    "    \n",
    "    # run face detector to get bounding rectangle\n",
    "    rect = detector(gray, 1)[0]\n",
    "    \n",
    "    # run landmark prediction on portion of image in face rectangle; output\n",
    "    shape = predictor(gray, rect)\n",
    "    shape_np = face_utils.shape_to_np(shape)\n",
    "    \n",
    "    return shape_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_norm_face(video, time, detector, predictor):\n",
    "    \"\"\"\n",
    "    Return a affine-transformed and rescaled face bounding box on the basis of the distance between the eye landmarks\n",
    "      and their angle relative to each other. Transformation centers the (pictured person's) left eye in a fixed location\n",
    "      in an ndarray of a fixed size and warps/resizes the image such that the right eye's center forms a horizontal line\n",
    "      with the left eye, and is a fixed distance away from it.\n",
    "    \"\"\"\n",
    "    # initialize util\n",
    "    detector = detector\n",
    "    predictor = predictor\n",
    "    fa = FaceAligner(predictor, desiredFaceWidth = 256) # TODO adjust as needed\n",
    "    \n",
    "    # get the raw frame\n",
    "    frame = get_video_frame(video, time)\n",
    "    frame = imutils.resize(frame, width=800) # TODO adjust as needed\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    rect = detector(gray,1)\n",
    "    \n",
    "    # ... TODO get this to work together\n",
    "    \n",
    "    # run imutils.resize on rect\n",
    "    #(x, y, w, h) = rect_to_bb(rect)\n",
    "    faceAligned = fa.align(frame, gray, rect)\n",
    "    \n",
    "    return faceAligned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function is heavily inspired by: http://codegists.com/snippet/python/facial_landmark_generatorpy_habanoz_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_landmarks(my_ndarray, shape, aperture_xy = False):\n",
    "    \"\"\"\n",
    "    Inputs: an ndarray frame output from a cv2.VideoCapture object, and a (68,2) ndarray of x,y coords that dlib detects.\n",
    "    Outputs: an image with lines drawn over the detected landmarks; useful for testing and visualization.\n",
    "    aperture_xy: if True, also draw (next to face) numerical values for x and y diameters of lip aperture.\n",
    "    \"\"\"\n",
    "\n",
    "    out_image = my_ndarray.copy()\n",
    "\n",
    "    for i,name in enumerate(face_utils.FACIAL_LANDMARKS_IDXS.keys()):\n",
    "        if name == \"mouth\":\n",
    "            continue\n",
    "        j,k = face_utils.FACIAL_LANDMARKS_IDXS[name]\n",
    "        pts = np.array(shape[j:k], dtype=np.uint32)\n",
    "        for idx,pt in enumerate(pts):\n",
    "            pt1 = pt\n",
    "            try:\n",
    "                pt2 = pts[idx+1]\n",
    "            except IndexError:\n",
    "                if name == \"left_eye\" or name == \"right_eye\":\n",
    "                    pt2 = pts[0]\n",
    "                else:\n",
    "                    continue\n",
    "            cv2.line(out_image, tuple(pt1), tuple(pt2), (255,255,255))\n",
    "    \n",
    "    # drawing the mouth with some more precision\n",
    "    # draw most of the outer perimeter of lips\n",
    "    jm,km = face_utils.FACIAL_LANDMARKS_IDXS['mouth']\n",
    "    for idx in range(jm,jm+11): \n",
    "        pt1 = shape[idx]\n",
    "        pt2 = shape[idx+1]\n",
    "        cv2.line(out_image, tuple(pt1), tuple(pt2), (255,255,255))\n",
    "    \n",
    "    # draw the last segment for the outer perimiter of lips\n",
    "    cv2.line(out_image, tuple(shape[48]), tuple(shape[59]), (255,255,255))\n",
    "    \n",
    "    # draw the inner aperture of the lips\n",
    "    for idx in range(jm+12,km):\n",
    "        pt1 = shape[idx]\n",
    "        try:\n",
    "            pt2 = shape[idx+1]\n",
    "        except IndexError:\n",
    "            pt2 = shape[jm+12]\n",
    "        cv2.line(out_image, tuple(pt1), tuple(pt2), (255,255,255))\n",
    "        \n",
    "    # add text indicating measured lip aperture in px\n",
    "    if aperture_xy:\n",
    "        x,y = get_lip_aperture(shape)\n",
    "        add_string = \"x={}, y={}\".format(round(x,1),round(y,1))\n",
    "        loc = tuple(np.subtract(shape[4], (200,0)))\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(out_image, add_string, loc, font, 0.8, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return out_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lip_aperture(shape):\n",
    "    \"\"\"\n",
    "    Inputs: the typical 68,2 ndarray shape object output by detect_landmarks.\n",
    "    Outputs: a 2-tuple of horizontal and vertical diameters of the lip aperture,\n",
    "     treating the horizontal line like the major axis of an ellipse,\n",
    "     and the vertical line like the minor axis.\n",
    "    \"\"\"\n",
    "    horizontal_axis = np.linalg.norm(shape[60] - shape[64])\n",
    "    vertical_axis = np.linalg.norm(shape[62] - shape[66])\n",
    "    \n",
    "    return horizontal_axis,vertical_axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lip_mask(frame, shape):\n",
    "    \"\"\"\n",
    "    Returns a simplified ndarray containing 0s/1s, with lips a filled polygon of 1s\n",
    "    \"\"\"\n",
    "    # fetch indices of mouth landmark points\n",
    "    jm,km = face_utils.FACIAL_LANDMARKS_IDXS['mouth']\n",
    "    \n",
    "    # initialize a blank background in the shape of the original image\n",
    "    mask_dims = [frame.shape[0],frame.shape[1]]\n",
    "    mask = np.zeros(ground_dims, dtype=np.uint8)\n",
    "\n",
    "    # fill outer lip polygon\n",
    "    mouth_outer = shape[jm:jm+11]\n",
    "    mouth_outer_col = [p[0] for p in mouth_outer]\n",
    "    mouth_outer_row = [p[1] for p in mouth_outer]\n",
    "    # last point's coords need to be appended manually\n",
    "    mouth_outer_col.append(shape[59][0])\n",
    "    mouth_outer_row.append(shape[59][1])\n",
    "    rr,cc = polygon(mouth_outer_row,mouth_outer_col)\n",
    "    mask[rr,cc] = 1\n",
    "\n",
    "    # then, cancel out inner polygon\n",
    "    mouth_inner = shape[jm+12:km]\n",
    "    mouth_inner_col = [p[0] for p in mouth_inner]\n",
    "    mouth_inner_row = [p[1] for p in mouth_inner]\n",
    "    rr,cc = polygon(mouth_inner_row,mouth_inner_col)\n",
    "    mask[rr,cc] = 0\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions in the two cells below are largely a product of Michelle Ching (with some modifications from me)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_center(image):\n",
    "    \"\"\" Returns the center location of an array as (x, y). \"\"\"\n",
    "    return np.array([image.shape[0] // 2, image.shape[1] // 2])\n",
    "  \n",
    "def centroid(image):\n",
    "    \"\"\" Returns centroid. \"\"\"\n",
    "    if (image == 0).all():\n",
    "        return get_center(image)\n",
    "    centroid = center_of_mass(image)\n",
    "    centroid = np.rint(np.array(centroid))\n",
    "    return centroid.astype(int)\n",
    "\n",
    "def cross(image, point):\n",
    "    \"\"\" Point-marking (with cross shape) function, to mark off i.e. centroid. \"\"\"\n",
    "    image_w, image_h = image.shape\n",
    "    x, y = point\n",
    "    blank = np.zeros(image.shape)\n",
    "    blank[max(0, x - 3):min(x + 4, image_w), y] = 1\n",
    "    blank[x, max(0, y - 3):min(y + 4, image_h)] = 1\n",
    "    blank[x,y] = 0\n",
    "    return blank.astype(bool).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crop_center(input_mask):\n",
    "    \"\"\" \n",
    "    Returns mask (1s/0s) with centroid of 1s centered on center of a smaller ground of 0s. \n",
    "    This normalizes for some aspects of head movement (relative size and absolute position),\n",
    "      but does not normalize for left/right head tilt.\n",
    "    \"\"\"\n",
    "    mask = input_mask.copy()\n",
    "    # define a smaller ground of zeros\n",
    "    # TODO make this scaled to the individual acquisition somehow, by e.g. lip width\n",
    "    ground_shape = np.array([int(d/6) for d in mask.shape])\n",
    "    ground = np.zeros(ground_shape)\n",
    "    \n",
    "    # get mask's centroid and ground's center point\n",
    "    cg_x, cg_y = get_center(ground)\n",
    "    mask_centroid = centroid(mask)\n",
    "    ci_x, ci_y = mask_centroid\n",
    "    # unpack dims of mask and ground\n",
    "    image_w, image_h = mask.shape\n",
    "    ground_w, ground_h = ground_shape\n",
    "    \n",
    "    # determine preliminary amounts to crop from left and top\n",
    "    left_diff = ci_x - cg_x\n",
    "    top_diff = ci_y - cg_y\n",
    "    \n",
    "    # check if left/top sides of image fit on ground; chop off if not\n",
    "    if left_diff > 0:\n",
    "        mask = mask[left_diff:,]\n",
    "        ci_x -= left_diff\n",
    "    if top_diff > 0:\n",
    "        mask = mask[:,top_diff:]\n",
    "        ci_y -= top_diff\n",
    "    \n",
    "    # get mask dims again\n",
    "    image_w, image_h = mask.shape\n",
    "        \n",
    "    # get right and bottom dimensions\n",
    "    ground_r = ground_w - cg_x\n",
    "    ground_b = ground_h - cg_y\n",
    "    image_r = image_w - ci_x\n",
    "    image_b = image_h - ci_y \n",
    "    \n",
    "    # determine amounts to crop from right and bottom\n",
    "    right_diff = ground_r - image_r\n",
    "    bottom_diff = ground_b - image_b\n",
    "    \n",
    "    # check if right/bottom sides of image fit on ground; chop off if not\n",
    "    if right_diff < 0:\n",
    "        mask = mask[:image_w + right_diff]\n",
    "    if bottom_diff < 0:\n",
    "        mask = mask[:,:image_h + bottom_diff]\n",
    "        \n",
    "    image_w, image_h = mask.shape\n",
    "\n",
    "    # copy modified mask onto ground shape\n",
    "    left_start = cg_x - ci_x\n",
    "    top_start = cg_y - ci_y\n",
    "    ground[left_start:left_start + image_w, top_start:top_start + image_h] += mask\n",
    "        \n",
    "    return ground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell will extract and display a single landmark-annotated video frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "video = 'SN12_AA032001.MXF'\n",
    "time = '00:00:10.900'\n",
    "\n",
    "frame = get_video_frame(video,time)\n",
    "shape = detect_landmarks(frame)\n",
    "color_corrected = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(draw_landmarks(color_corrected,shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This (slightly messy) cell extracts a frame corresponding to the desired timepoint of an audio file (here, from a TextGrid) and extracts a polygon of the lip shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "babb = 'vowels/P1.MXF'\n",
    "babb_tg = os.path.splitext(babb)[0] + \".TextGrid\"\n",
    "pm = audiolabel.LabelManager(from_file=babb_tg, from_type=\"praat\")\n",
    "v = pm.tier('phone').search(vre)[0] # should only be one match per file; TODO check while running\n",
    "midpoint= v.center\n",
    "ffmpeg_time = str(round(midpoint,3))\n",
    "frame = get_video_frame(babb,ffmpeg_time)\n",
    "shape = detect_landmarks(frame)\n",
    "color_corrected = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(draw_landmarks(color_corrected,shape))\n",
    "\n",
    "image = lip_mask(frame,shape)\n",
    "plt.imshow(crop_center(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell clips a short stretch of video from the longer videos typical of ultrasound acquisitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp_movie = '12-video/SN12_AA032001.MXF'\n",
    "exp_snippet = '12-video/SN12_AA032001_short.MXF'\n",
    "fname = os.path.split(exp_movie)[1]\n",
    "basename = os.path.splitext(fname)[0]\n",
    "\n",
    "# get a snippet of the MXF\n",
    "snippet_args = ['ffmpeg', '-ss', '00:00:30', \n",
    "                '-i', exp_movie, \n",
    "                \"-t\", \"00:00:08\", \n",
    "                \"-vcodec\", \"copy\", \n",
    "                \"-acodec\", \"copy\", \n",
    "                exp_snippet]\n",
    "subprocess.check_call(snippet_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this cell takes a short clip and creates (and saves) a GIF with the facial landmarks drawn over the subject's face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_movie = 'vowels/POO1.MXF'\n",
    "fname = os.path.split(exp_movie)[1]\n",
    "basename = os.path.splitext(fname)[0]\n",
    "\n",
    "subprocess.check_call(['ffmpeg','-loglevel','8','-i',exp_snippet,'-vf','fps=20','img_%05d_f.bmp'])\n",
    "movie = cv2.VideoCapture('img_%05d_f.bmp')\n",
    "\n",
    "frame_num = 1\n",
    "\n",
    "while(movie.isOpened()):\n",
    "    ret, frame = movie.read()\n",
    "    if (ret==False):   # the file is finished\n",
    "        break\n",
    "\n",
    "    # detect face region and draw landmarks on the image.\n",
    "    shape = detect_landmarks(frame)\n",
    "    out_image = draw_landmarks(frame,shape)\n",
    "    \n",
    "    # write the image to a .bmp file, with zero-padding to ensure the frames are input \n",
    "    cv2.imwrite('{0:05d}g.bmp'.format(frame_num), out_image)\n",
    "    frame_num += 1\n",
    "        \n",
    "# cleanup\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# make a gif (a bit slowed down by default) from the output bmps\n",
    "subprocess.check_call(['convert', '*g.bmp', basename+'.gif'])\n",
    "\n",
    "# remove the input bmps\n",
    "for bmp in glob.glob(\"*g.bmp\"):\n",
    "    os.remove(bmp)\n",
    "for bmp in glob.glob(\"*f.bmp\"):\n",
    "    os.remove(bmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Development area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YET TO DO\n",
    "# tilt normalization\n",
    "# get all lip shape outputs and store in ndarray\n",
    "# for each lip shape object (slice of ndarray):\n",
    "#     get centroid/mean\n",
    "#     move these to sufficiently large ground (on subject-to-subject basis) centered on ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
